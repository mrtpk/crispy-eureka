{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 0\n",
    "- The objective with this experiment is to test the pipeline devloped.\n",
    "\n",
    "## TODOs\n",
    "1. Use fit_generator() instead of generator()\n",
    "2. Use H5py format to store features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import sklearn.model_selection as sk\n",
    "import copy\n",
    "from PIL import Image as IM\n",
    "from keras.callbacks import TensorBoard\n",
    "import cv2\n",
    "import json\n",
    "import pathlib\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import helpers\n",
    "from helpers import data_loaders as dls\n",
    "from helpers import pointcloud as pc\n",
    "from helpers.viz import plot, plot_history\n",
    "from helpers.logger import Logger\n",
    "import utils\n",
    "## import networks\n",
    "import lodnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../' # path of the repo.\n",
    "_NAME = 'experiment0' # name of experiment\n",
    "!ls $PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is better to create a folder with runid in the experiment folder\n",
    "_EXP, _LOG, _TMP, _RUN_PATH = dls.create_dir_struct(PATH, _NAME)\n",
    "logger = Logger('EXP0', _LOG + 'experiment0.log')\n",
    "logger.debug('Logger EXP0 int')\n",
    "!ls $_EXP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset\n",
    "train_set, valid_set, test_set = dls.get_dataset(PATH, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KPC = utils.KittiPointCloudClass(train_set=train_set, valid_set=valid_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments\n",
    "- Use the LoDNN's Ground truth since its better [look at [LoDNN paper](https://arxiv.org/pdf/1703.03613.pdf) Figure 3]\n",
    "- plot histograms over the normalized features, observe how this changes once we start subsampling\n",
    "- Use vanilla U-net implementation from [[here](https://github.com/karolzak/keras-unet#Vanilla-U-Net)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_index = -1\n",
    "# NOTE: change limit_index to -1 to train on the whole dataset\n",
    "f_train = dls.process_pc(train_set[\"pc\"][0:limit_index], lambda x: KPC.get_features(x))\n",
    "f_valid = dls.process_pc(valid_set[\"pc\"][0:limit_index], lambda x: KPC.get_features(x))\n",
    "f_test = dls.process_pc(test_set[\"pc\"][0:limit_index], lambda x: KPC.get_features(x))\n",
    "gt_train = dls.process_img(train_set[\"gt_bev\"][0:limit_index], func=lambda x: utils.kitti_gt(x))\n",
    "gt_valid = dls.process_img(valid_set[\"gt_bev\"][0:limit_index], func=lambda x: utils.kitti_gt(x))\n",
    "gt_test = dls.process_img(train_set[\"gt_bev\"][0:limit_index], func=lambda x: utils.kitti_gt(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = plot([[f_train[0][:,:,0], f_train[1][:,:,0],  f_train[2][:,:,0]],\n",
    "#       [f_train[0][:,:,1], f_train[1][:,:,1],  f_train[2][:,:,1]],\n",
    "#       [gt_train[0][:,:,0], gt_train[1][:,:,0], gt_train[1][:,:,0]]\n",
    "#      ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let the training begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_id():\n",
    "    return datetime.datetime.today().strftime('%Y_%m_%d_%H_%M')\n",
    "\n",
    "def create_run_dir(path, run_id):\n",
    "    path = path.replace(\"*run_id\", str(run_id))\n",
    "    model_dir = \"{}/model/\".format(path)\n",
    "    output_dir = \"{}/output/\".format(path)\n",
    "    log_dir = \"{}/log/\".format(path)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basic_callbacks(path):\n",
    "    # tensorboard\n",
    "    # to visualise `tensorboard --logdir=\"./path_to_log_dir\" --port 6006`\n",
    "    log_path = \"{}/log/\".format(path)\n",
    "    tensorboard = TensorBoard(log_dir=\"{}/{}\".format(log_path, time()))\n",
    "    # save best model\n",
    "    best_model_path = \"{}/model/best_model.h5\".format(path) #? .hd5\n",
    "    save_the_best = keras.callbacks.ModelCheckpoint(filepath=best_model_path,\n",
    "                                                    verbose=1, save_best_only=True)\n",
    "    # save models after few epochs\n",
    "    epoch_save_path = \"{}/model/*.h5\".format(path)\n",
    "    save_after_epoch = keras.callbacks.ModelCheckpoint(filepath=epoch_save_path.replace('*', 'e{epoch:02d}-val_acc{val_acc:.2f}'),\n",
    "                                                       monitor='val_acc', verbose=1, period = 1)\n",
    "    return [tensorboard, save_the_best, save_after_epoch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = get_unique_id()\n",
    "path = create_run_dir(_RUN_PATH, run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lodnn.get_model()\n",
    "# model.summary()\n",
    "callbacks = get_basic_callbacks(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more callbacks\n",
    "# early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "# reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.000001)\n",
    "# callbacks = callbacks + [early_stopping, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All training params to be added here\n",
    "training_config = {\n",
    "    \"loss_function\" : \"binary_crossentropy\",\n",
    "    \"learning_rate\" : 1e-4,\n",
    "    \"batch_size\"    : 1,\n",
    "    \"epochs\"        : 10,\n",
    "    \"optimizer\"     : \"keras.optimizers.Adam\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = eval(training_config[\"optimizer\"])(lr=training_config[\"learning_rate\"])\n",
    "model.compile(loss=training_config[\"loss_function\"],\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add fit_generator\n",
    "m_history = model.fit(x=np.array(f_train),\n",
    "                      y=np.array(gt_train),\n",
    "                      batch_size=training_config[\"batch_size\"],\n",
    "                      epochs=training_config[\"epochs\"],\n",
    "                      verbose=1,\n",
    "                      callbacks=callbacks,\n",
    "                      validation_data=(np.array(f_valid), np.array(gt_valid))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"{}/model/final_model.h5\".format(path))\n",
    "plot_history(m_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # predict for one sample\n",
    "# f_pc_test = dls.process_pc(test_set[\"pc\"][0:1], lambda x: _get_features(x))\n",
    "# f_pc_test = np.expand_dims(f_pc_test[0], axis=0)\n",
    "# res_test = model.predict(f_pc_test, verbose=1).squeeze()\n",
    "# plot([[res_test[:, : , 0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(model, features):\n",
    "    '''\n",
    "    Get prediction for a given model and point cloud.\n",
    "    '''\n",
    "    # f = utils._get_features(points)\n",
    "    f = np.expand_dims(features, axis=0)\n",
    "    return model.predict(f, verbose=0).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = dls.process_pc(test_set[\"pc\"][0:1], lambda x:x)[0]\n",
    "features = utils._get_features(points)\n",
    "res = get_prediction(model, features)\n",
    "plot([[res[:, : ,0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_threshold(res, threshold=0.9):\n",
    "    '''\n",
    "    Apply thresholding on probability values.\n",
    "    '''\n",
    "    _tmp = res.copy()\n",
    "    _tmp[res >= threshold] = 1\n",
    "    _tmp[res < threshold] = 0\n",
    "    return _tmp.astype(np.uint8)\n",
    "\n",
    "def apply_argmax(res):\n",
    "    return np.argmax(res, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binarization of softmax output from LODNN\n",
    "- Ideally this threshold needs to be determined by the classifier itself and we perform argmax, for now we can stay with argmax\n",
    "- An overlay between the prediction and ground truth would be useful to see where the errors are\n",
    "- Maybe add post processing with morphological filling/reconstruction filters ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.4\n",
    "plot([[apply_threshold(res, THRESHOLD)[:, : ,0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Argmax from softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([[1-apply_argmax(res)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(path, model, dataset, threshold, is_viz=False):\n",
    "    result_path = \"{}/output/*\".format(path)\n",
    "    os.makedirs(os.path.dirname(result_path), exist_ok=True)\n",
    "    # NOTE: Now computing for 8 samples (can this be done on samples from the train and validation set?)\n",
    "    f = dls.process_pc(dataset[\"pc\"][0:8], lambda x: _get_features(x))\n",
    "    gt = dls.process_img(dataset[\"gt_bev\"][0:8], func=lambda x: kitti_gt(x))\n",
    "    F1, P, R, ACC = [], [], [], []\n",
    "    FN, FP, TP, TN = [], [], [], []\n",
    "    counter = 0\n",
    "    diplay_thresh = 3\n",
    "    display = []\n",
    "    labels = []\n",
    "    \n",
    "    for i, datum in enumerate(zip(f, gt)):\n",
    "        _f, _gt = datum\n",
    "        res = get_prediction(model, _f)\n",
    "        th_res = apply_threshold(res, threshold=threshold)\n",
    "               \n",
    "        # get metrics\n",
    "        p_road = th_res[:, :, 0]\n",
    "        gt_road = _gt[:, :, 0]\n",
    "        fn, fp, tp, tn = utils.get_metrics_count(pred=p_road, gt=gt_road)\n",
    "        f1, recall, precision, acc = utils.get_metrics(gt=gt_road, pred=p_road)\n",
    "        F1.append(f1)\n",
    "        P.append(precision)\n",
    "        R.append(recall)\n",
    "        ACC.append(acc)\n",
    "        TP.append(tp)\n",
    "        FP.append(fp)\n",
    "        TN.append(tn)\n",
    "        FN.append(fn)\n",
    "        \n",
    "        # for viz\n",
    "        if is_viz is True:\n",
    "            display.append([gt_road, res[:,:,0], p_road])\n",
    "            labels.append([\"gt\", \"Acc: {}\".format(str(np.round(acc,2))), \"F1: {}\".format(str(np.round(f1,2)))])\n",
    "            if (i+1) % diplay_thresh == 0:\n",
    "                plot(display, labels, fontsize=10)\n",
    "                plt.savefig(result_path.replace(\"*\", str(i+1) + \".jpg\"))\n",
    "                display, labels = [], []\n",
    "    if is_viz is True:\n",
    "        plot(display, labels, fontsize=10)\n",
    "        plt.savefig(result_path.replace(\"*\", str(i+1) + \".jpg\"))\n",
    "    \n",
    "    eps = np.finfo(np.float32).eps        \n",
    "    _acc = (sum(TP)+sum(TN))/(sum(TP)+sum(FP)+sum(TN)+sum(FN) + eps)\n",
    "    _recall = sum(TP)/(sum(TP) + sum(FN)+eps)\n",
    "    _precision = sum(TP)/(sum(TP) + sum(FP)+eps)\n",
    "    _f1 = 2*((precision * recall)/(precision + recall))    \n",
    "    return _acc, _recall, _precision, _f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_acc, _recall, _precision, _f1 = get_output(path, model, test_set, threshold=THRESHOLD, is_viz=True)\n",
    "print(_acc, _recall, _precision, _f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_acc, _recall, _precision, _f1 = get_output(path, model, test_set, threshold=THRESHOLD, is_viz=True, sklearn_flag=True)\n",
    "print(_acc, _recall, _precision, _f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "details = {\"name\" : _NAME,\n",
    "           \"run_id\" : run_id,\n",
    "           \"dataset\": \"KITTI\",\n",
    "           \"training_config\" : training_config,\n",
    "           \"threshold\" : THRESHOLD,\n",
    "           \"accuracy\" : _acc,\n",
    "           \"recall\" : _recall,\n",
    "           \"precision\" : _precision,\n",
    "           \"F1\" : _f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}/details.json'.format(path), 'w') as f:\n",
    "    json.dump(details, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
